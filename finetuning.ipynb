{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning LLM"
      ],
      "metadata": {
        "id": "ZRTECwpFJ-kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch version: 2.0.1+cu117\n",
        "# Transformers version: 4.31.0"
      ],
      "metadata": {
        "id": "oDJLgp71KATp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "J5_lG02zLgTv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6o5fLrsI_MT",
        "outputId": "1612490c-7d08-4382-cd8f-a79acb9616de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu117\n",
            "Transformers version: 4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datasets\n",
        "import os\n",
        "import lamini\n",
        "from pprint import pprint\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "import config\n",
        "import yaml\n",
        "import time\n",
        "import jsonlines\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoModelForCausalLM\n",
        "from llama import BasicModelRunner\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None"
      ],
      "metadata": {
        "id": "33JFXwZYJgPx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data prep"
      ],
      "metadata": {
        "id": "bYFEIQgLLq6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdNbaNwiLqIP",
        "outputId": "0c53d73b-8b64-41ea-f98b-c00006b2d44d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hi, how are you?\"\n",
        "encoded_text = tokenizer(text)[\"input_ids\"]\n",
        "encoded_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a-dJvkvLlhc",
        "outputId": "8cdabcdd-4bda-41a1-fc10-a6249da1c310"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12764, 13, 849, 403, 368, 32]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(encoded_text)\n",
        "print(\"Decoded tokens back into text: \", decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNd955FELy8l",
        "outputId": "a956afe4-e064-4056-fa03-4b09e2ada784"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded tokens back into text:  Hi, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
        "encoded_texts = tokenizer(list_texts)\n",
        "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NSWTbIyRUuw",
        "outputId": "db4ac096-a267-4f35-b264-7e30143ed780"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded several texts:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
        "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVQUt4WeL05Z",
        "outputId": "faadecbf-cca9-4493-a82a-1f000e509369"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using padding:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175, 0, 0, 0], [4374, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
        "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTmB5h03L5aF",
        "outputId": "cef4cec5-7082-4d86-ad6d-d01d3a65a8d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using truncation:  [[12764, 13, 849], [42, 1353, 1175], [4374]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.truncation_side = \"left\"\n",
        "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
        "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvq1z8ECL_2i",
        "outputId": "d7614394-1e05-4c62-c540-4ec2788dc311"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using left-side truncation:  [[403, 368, 32], [42, 1353, 1175], [4374]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
        "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj7_pf0JMA9J",
        "outputId": "12de875e-34b6-44c8-a940-fcb9a4691ec7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using both padding and truncation:  [[403, 368, 32], [42, 1353, 1175], [4374, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"lamini_docs.jsonl\"\n",
        "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
        "examples = instruction_dataset_df.to_dict()\n",
        "\n",
        "if \"question\" in examples and \"answer\" in examples:\n",
        "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "elif \"instruction\" in examples and \"response\" in examples:\n",
        "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
        "elif \"input\" in examples and \"output\" in examples:\n",
        "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "else:\n",
        "  text = examples[\"text\"][0]\n",
        "\n",
        "prompt_template = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "\n",
        "num_examples = len(examples[\"question\"])\n",
        "finetuning_dataset = []\n",
        "for i in range(num_examples):\n",
        "  question = examples[\"question\"][i]\n",
        "  answer = examples[\"answer\"][i]\n",
        "  text_with_prompt_template = prompt_template.format(question=question)\n",
        "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
        "\n",
        "from pprint import pprint\n",
        "print(\"One datapoint in the finetuning dataset:\")\n",
        "pprint(finetuning_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgBNhjVjMCch",
        "outputId": "22c93eb5-b111-4717-9e0d-31b75017ce01"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One datapoint in the finetuning dataset:\n",
            "{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
            "           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
            "           'Advanced topics, and class documentation on LLM Engine available '\n",
            "           'at https://lamini-ai.github.io/.',\n",
            " 'question': '### Question:\\n'\n",
            "             'What are the different types of documents available in the '\n",
            "             'repository (e.g., installation guide, API documentation, '\n",
            "             \"developer's guide)?\\n\"\n",
            "             '\\n'\n",
            "             '### Answer:'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
        "tokenized_inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"np\",\n",
        "    padding=True\n",
        ")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_JzXmJNM3_8",
        "outputId": "2b682bf8-c352-44a4-809d-32f26b15af57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4118 19782    27   187  1276   403   253  1027  3510   273  7177  2130\n",
            "    275   253 18491   313    70    15    72   904 12692  7102    13  8990\n",
            "  10097    13 13722   434  7102  6177   187   187  4118 37741    27    45\n",
            "   4988    74   556 10097   327 27669 11075   264    13  5271 23058    13\n",
            "  19782 37741 10031    13 13814 11397    13   378 16464    13 11759 10535\n",
            "   1981    13 21798 12989    13   285   966 10097   327 21708    46 10797\n",
            "   2130   387  5987  1358    77  4988    74    14  2284    15  7280    15\n",
            "    900 14206]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 2048\n",
        "max_length = min(\n",
        "    tokenized_inputs[\"input_ids\"].shape[1],\n",
        "    max_length,\n",
        ")"
      ],
      "metadata": {
        "id": "0vS40IBKNCQw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"np\",\n",
        "    truncation=True,\n",
        "    max_length=max_length\n",
        ")"
      ],
      "metadata": {
        "id": "Qfss8DfBNFO8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs[\"input_ids\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dCQ38UnNGam",
        "outputId": "bd0b5f64-bdfd-4181-e966-9bda2f99a743"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4118, 19782,    27,   187,  1276,   403,   253,  1027,  3510,\n",
              "          273,  7177,  2130,   275,   253, 18491,   313,    70,    15,\n",
              "           72,   904, 12692,  7102,    13,  8990, 10097,    13, 13722,\n",
              "          434,  7102,  6177,   187,   187,  4118, 37741,    27,    45,\n",
              "         4988,    74,   556, 10097,   327, 27669, 11075,   264,    13,\n",
              "         5271, 23058,    13, 19782, 37741, 10031,    13, 13814, 11397,\n",
              "           13,   378, 16464,    13, 11759, 10535,  1981,    13, 21798,\n",
              "        12989,    13,   285,   966, 10097,   327, 21708,    46, 10797,\n",
              "         2130,   387,  5987,  1358,    77,  4988,    74,    14,  2284,\n",
              "           15,  7280,    15,   900, 14206]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    if \"question\" in examples and \"answer\" in examples:\n",
        "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "    elif \"input\" in examples and \"output\" in examples:\n",
        "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "    else:\n",
        "      text = examples[\"text\"][0]\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    max_length = min(\n",
        "        tokenized_inputs[\"input_ids\"].shape[1],\n",
        "        2048\n",
        "    )\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "THlfaYVyNHzz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
        "\n",
        "tokenized_dataset = finetuning_dataset_loaded.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True\n",
        ")\n",
        "\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy49iNK2NKTU",
        "outputId": "f7115f2c-1b92-4f4b-ecc0-36c38cfca231"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 1400\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
      ],
      "metadata": {
        "id": "mFtgPwQ5NNul"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
        "print(split_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F64H8G6NOrk",
        "outputId": "7d4c460b-6df0-457d-e4fc-4f0ed3bc8b5f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1260\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 140\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "mo0DFmeDNg8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llama import BasicModelRunner\n",
        "\n",
        "# model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n",
        "# model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
        "# model.train(is_public=True)"
      ],
      "metadata": {
        "id": "jn83uxlHNQrQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lamini.api_url = os.getenv(\"POWERML__PRODUCTION__URL\")\n",
        "lamini.api_key = os.getenv(\"POWERML__PRODUCTION__KEY\")"
      ],
      "metadata": {
        "id": "JWsBJ0iQNiyU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lamini docs dataset"
      ],
      "metadata": {
        "id": "7_NjD0mROHDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"lamini/lamini_docs\"\n",
        "use_hf = True"
      ],
      "metadata": {
        "id": "ehHoWzFJOKEb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model, training config, tokenizer setup"
      ],
      "metadata": {
        "id": "5WZWMBcbOR4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_config_and_logging(existing_config=None):\n",
        "    global global_config\n",
        "    global_config = build_config(existing_config)\n",
        "    setup_logging(global_config)\n",
        "    logger.debug(\"Config: \" + str(yaml.dump(global_config.as_dict())))\n",
        "    return global_config\n",
        "\n",
        "def get_config():\n",
        "    global global_config\n",
        "    assert global_config is not None\n",
        "    return global_config\n",
        "\n",
        "def build_config(existing_config=None):\n",
        "    configs = [\n",
        "        # Using config library\n",
        "        config.config_from_env(prefix=\"LLAMA\", separator=\"_\", lowercase_keys=True),\n",
        "    ]\n",
        "\n",
        "    if existing_config:\n",
        "        if isinstance(existing_config, dict):\n",
        "            configs.append(config.config_from_dict(existing_config))\n",
        "        else:\n",
        "            configs.append(existing_config)\n",
        "\n",
        "    config_paths = get_config_paths()\n",
        "\n",
        "    for path in reversed(config_paths):\n",
        "        print(\"Loading builtin config from \" + path)\n",
        "        configs.append(config.config_from_yaml(path, read_from_file=True))\n",
        "\n",
        "    return config.ConfigurationSet(*configs)\n",
        "\n",
        "def get_config_paths():\n",
        "    paths = []\n",
        "\n",
        "def get_config_paths():\n",
        "    paths = []\n",
        "\n",
        "    config_name = \"llama_config\"\n",
        "    config_base = \"configs\"\n",
        "\n",
        "    base_config_path = os.path.join(config_base, config_name + \".yaml\")\n",
        "    if os.path.exists(base_config_path):\n",
        "        paths.append(base_config_path)\n",
        "\n",
        "    local_config_path = os.path.join(config_base, config_name + \"_local.yaml\")\n",
        "    if os.path.exists(local_config_path):\n",
        "        paths.append(local_config_path)\n",
        "\n",
        "    home = os.path.expanduser(\"~\")\n",
        "    home_config_path = os.path.join(home, \".\" + config_name + \".yaml\")\n",
        "    if os.path.exists(home_config_path):\n",
        "        paths.append(home_config_path)\n",
        "\n",
        "    return paths\n",
        "\n",
        "def setup_logging(arguments):\n",
        "    logging_format = \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"\n",
        "\n",
        "    if arguments[\"verbose\"]:\n",
        "        logging.basicConfig(level=logging.DEBUG, format=logging_format)\n",
        "    elif arguments[\"verbose_info\"]:\n",
        "        logging.basicConfig(level=logging.INFO, format=logging_format)\n",
        "    else:\n",
        "        logging.basicConfig(level=logging.WARNING, format=logging_format)\n",
        "\n",
        "    root_logger = logging.getLogger()\n",
        "\n",
        "    if arguments[\"verbose\"]:\n",
        "        root_logger.setLevel(logging.DEBUG)\n",
        "    elif arguments[\"verbose_info\"]:\n",
        "        root_logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        root_logger.setLevel(logging.WARNING)\n",
        "\n",
        "    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"smart_open\").setLevel(logging.WARNING)\n",
        "    logging.getLogger(\"botocore\").setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "yr7NwkpoOny1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper for data load, split, tokenize for training\n",
        "def tokenize_and_split_data(training_config, tokenizer):\n",
        "  initialized_config = initialize_config_and_logging(training_config)\n",
        "  dataset_path = initialized_config[\"datasets\"][\"path\"]\n",
        "  use_hf = initialized_config[\"datasets\"][\"use_hf\"]\n",
        "  print(\"tokenize\", use_hf, dataset_path)\n",
        "  if use_hf:\n",
        "    dataset = datasets.load_dataset(dataset_path)\n",
        "  else:\n",
        "    dataset = load_dataset(dataset_path, tokenizer)\n",
        "  train_dataset = dataset[\"train\"]\n",
        "  test_dataset = dataset[\"test\"]\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "# Tokenize and split data\n",
        "def load_dataset(dataset_path, tokenizer):\n",
        "    random.seed(42)\n",
        "    finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    max_length = training_config[\"model\"][\"max_length\"]\n",
        "    tokenized_dataset = finetuning_dataset_loaded.map(\n",
        "        get_tokenize_function(tokenizer, max_length), # returns tokenize_function\n",
        "        batched=True,\n",
        "        batch_size=1,\n",
        "        drop_last_batch=True\n",
        "    )\n",
        "    tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
        "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
        "    return split_dataset\n",
        "\n",
        "# Get function for tokenization, based on config parameters\n",
        "def get_tokenize_function(tokenizer, _max_length):\n",
        "\n",
        "  def tokenize_function(examples):\n",
        "    max_length = _max_length\n",
        "\n",
        "    # Set pad token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    if \"question\" in examples and \"answer\" in examples:\n",
        "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "    elif \"input\" in examples and \"output\" in examples:\n",
        "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "    else:\n",
        "      text = examples[\"text\"][0]\n",
        "\n",
        "    # Run tokenizer on all the text (the input and the output)\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "\n",
        "        # Return tensors in a numpy array (other options are pytorch or tf objects)\n",
        "        return_tensors=\"np\",\n",
        "\n",
        "        # Padding type is to pad to the longest sequence in the batch (other option is to a certain max length, or no padding)\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    # Calculate max length\n",
        "    max_length = min(\n",
        "        tokenized_inputs[\"input_ids\"].shape[1],\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    if tokenized_inputs[\"input_ids\"].shape[1] > max_length:\n",
        "        logger.warn(\n",
        "            f\"Truncating input from {tokenized_inputs['input_ids'].shape[1]} to {max_length}\"\n",
        "        )\n",
        "\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
        "\n",
        "    return tokenized_inputs\n",
        "  return tokenize_function"
      ],
      "metadata": {
        "id": "qE5unt-IOhOS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"EleutherAI/pythia-70m\"\n",
        "\n",
        "training_config = {\n",
        "    \"model\": {\n",
        "        \"pretrained_name\": model_name,\n",
        "        \"max_length\" : 2048\n",
        "    },\n",
        "    \"datasets\": {\n",
        "        \"use_hf\": use_hf,\n",
        "        \"path\": dataset_path\n",
        "    },\n",
        "    \"verbose\": True\n",
        "}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HH7d-KQOVDk",
        "outputId": "e639c121-8459-4060-cae3-62044d9e2af5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:__main__:Config: datasets.path: lamini/lamini_docs\n",
            "datasets.use_hf: true\n",
            "model.max_length: 2048\n",
            "model.pretrained_name: EleutherAI/pythia-70m\n",
            "verbose: true\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenize True lamini/lamini_docs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:fsspec.local:open file: /root/.cache/huggingface/datasets/lamini___lamini_docs/default/0.0.0/05bd680b81d69a7a1d38193873f1487d73e535bf/dataset_info.json\n",
            "DEBUG:fsspec.local:open file: /root/.cache/huggingface/datasets/lamini___lamini_docs/default/0.0.0/05bd680b81d69a7a1d38193873f1487d73e535bf/dataset_info.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 1260\n",
            "})\n",
            "Dataset({\n",
            "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 140\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base model"
      ],
      "metadata": {
        "id": "fgCwTtJzOvRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    logger.debug(\"Select GPU device\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    logger.debug(\"Select CPU device\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "base_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9viKbsNAOioH",
        "outputId": "5de53efd-fb9b-471e-c9ce-17a4f2f51dae"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:__main__:Select GPU device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50304, 512)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "SswkuOyoO0T1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = test_dataset[0]['question']\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
        "print(\"Model's answer: \")\n",
        "print(inference(test_text, base_model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoAq3O_hO4km",
        "outputId": "a74217b3-ac81-42dc-ef10-9a948a306371"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
            "Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
            "Model's answer: \n",
            "\n",
            "\n",
            "I have a question about the following:\n",
            "\n",
            "How do I get the correct documentation to work?\n",
            "\n",
            "A:\n",
            "\n",
            "I think you need to use the following code:\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the following code to get the correct documentation.\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the following code to get the correct documentation.\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the following\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "D5QLP-jdPC1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 3\n",
        "\n",
        "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
        "output_dir = trained_model_name\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  learning_rate=1.0e-5,\n",
        "  num_train_epochs=1,\n",
        "  max_steps=max_steps,\n",
        "  per_device_train_batch_size=1,\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  overwrite_output_dir=False,\n",
        "  disable_tqdm=False,\n",
        "  eval_steps=120,\n",
        "  save_steps=120,\n",
        "  warmup_steps=1,\n",
        "  per_device_eval_batch_size=1,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")\n",
        "\n",
        "model_flops = (\n",
        "  base_model.floating_point_ops(\n",
        "    {\n",
        "       \"input_ids\": torch.zeros(\n",
        "           (1, training_config[\"model\"][\"max_length\"])\n",
        "      )\n",
        "    }\n",
        "  )\n",
        "  * training_args.gradient_accumulation_steps\n",
        ")\n",
        "\n",
        "print(base_model)\n",
        "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
        "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8p6yFW7O6Kj",
        "outputId": "b91bcf71-c733-4326-c701-e73af3aff724"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTNeoXForCausalLM(\n",
            "  (gpt_neox): GPTNeoXModel(\n",
            "    (embed_in): Embedding(50304, 512)\n",
            "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x GPTNeoXLayer(\n",
            "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (attention): GPTNeoXAttention(\n",
            "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
            "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
            "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (mlp): GPTNeoXMLP(\n",
            "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (act): GELUActivation()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
            ")\n",
            "Memory footprint 0.30687256 GB\n",
            "Flops 2195.667812352 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(transformers.Trainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        model_flops,\n",
        "        total_steps,\n",
        "        args=None,\n",
        "        data_collator=None,\n",
        "        train_dataset=None,\n",
        "        eval_dataset=None,\n",
        "        tokenizer=None,\n",
        "        model_init=None,\n",
        "        compute_metrics=None,\n",
        "        callbacks=None,\n",
        "        optimizers=(None, None),\n",
        "    ):\n",
        "        super(Trainer, self).__init__(\n",
        "            model,\n",
        "            args,\n",
        "            data_collator,\n",
        "            train_dataset,\n",
        "            eval_dataset,\n",
        "            tokenizer,\n",
        "            model_init,\n",
        "            compute_metrics,\n",
        "            callbacks,\n",
        "            optimizers,\n",
        "        )\n",
        "\n",
        "        self.total_steps = total_steps\n",
        "        self.model_flops = model_flops\n",
        "        self.start_step = 0\n",
        "\n",
        "    def training_step(self, model, inputs):\n",
        "        if inputs[\"input_ids\"].numel() == 0:\n",
        "\n",
        "          print(\"Inputs: \", inputs)\n",
        "          print(\"Inputs - input_ids\", inputs[\"input_ids\"])\n",
        "          print(\"numel\", inputs[\"input_ids\"].numel())\n",
        "\n",
        "          return torch.tensor(0)\n",
        "        else:\n",
        "          model.train()\n",
        "          inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "          with self.compute_loss_context_manager():\n",
        "              loss = self.compute_loss(model, inputs)\n",
        "\n",
        "          if self.args.n_gpu > 1:\n",
        "              loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "\n",
        "          if self.do_grad_scaling:\n",
        "              self.scaler.scale(loss).backward()\n",
        "          else:\n",
        "              self.accelerator.backward(loss)\n",
        "\n",
        "          return loss.detach() / self.args.gradient_accumulation_steps\n",
        "\n",
        "    def log(self, logs):\n",
        "        \"\"\"\n",
        "        Log `logs` on the various objects watching training.\n",
        "        Subclass and override this method to inject custom behavior.\n",
        "        Args:\n",
        "            logs (`Dict[str, float]`):\n",
        "                The values to log.\n",
        "        \"\"\"\n",
        "        if self.state.epoch is not None:\n",
        "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
        "\n",
        "        self.update_log_timing(logs)\n",
        "\n",
        "        output = {**logs, **{\"step\": self.state.global_step}}\n",
        "        self.update_history(output)\n",
        "\n",
        "        logger.debug(\"Step (\" + str(self.state.global_step) + \") Logs: \" + str(logs))\n",
        "        self.control = self.callback_handler.on_log(\n",
        "            self.args, self.state, self.control, logs\n",
        "        )\n",
        "\n",
        "    def update_log_timing(self, logs):\n",
        "        if len(self.state.log_history) == 0:\n",
        "            self.start_time = time.time()\n",
        "            logs[\"iter_time\"] = 0.0\n",
        "            logs[\"flops\"] = 0.0\n",
        "            logs[\"remaining_time\"] = 0.0\n",
        "            self.start_step = self.state.global_step\n",
        "        elif self.state.global_step > self.start_step:\n",
        "            logs[\"iter_time\"] = (time.time() - self.start_time) / (\n",
        "                self.state.global_step - self.start_step\n",
        "            )\n",
        "            logs[\"flops\"] = self.model_flops / logs[\"iter_time\"]\n",
        "            logs[\"remaining_time\"] = (self.total_steps - self.state.global_step) * logs[\n",
        "                \"iter_time\"\n",
        "            ]\n",
        "\n",
        "    def update_history(self, output):\n",
        "        if \"eval_loss\" in output:\n",
        "            return\n",
        "        if len(self.state.log_history) > 0:\n",
        "            smoothing_window = 100\n",
        "            p = 1.0 / smoothing_window\n",
        "            if \"loss\" in output:\n",
        "                output[\"loss\"] = output[\"loss\"] * p + self.state.log_history[-1][\n",
        "                    \"loss\"\n",
        "                ] * (1.0 - p)\n",
        "        self.state.log_history.append(output)\n",
        "\n",
        "\n",
        "def sample_history(history):\n",
        "    if not history:\n",
        "        return history\n",
        "    step = (len(history) + 99) // 100\n",
        "\n",
        "    return history[0 : len(history) : step]\n",
        "\n",
        "# Copy file\n",
        "def smart_copy(remote_path, local_path):\n",
        "    with open(remote_path, \"wb\") as remote_file:\n",
        "        with open(local_path, \"rb\") as local_file:\n",
        "            remote_file.write(local_file.read())"
      ],
      "metadata": {
        "id": "Dtij8BaZSIqX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    model_flops=model_flops,\n",
        "    total_steps=max_steps,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "wse79XNHPcI3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = f'{output_dir}/final'\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV7Fn-8mR9iZ",
        "outputId": "114b1ba0-ea2e-45d0-dfc8-06492e149107"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to: lamini_docs_3_steps/final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
      ],
      "metadata": {
        "id": "Awp-dvIKSNf8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_slightly_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOBS-oy0Sg3i",
        "outputId": "27fa1e7f-db6e-42ff-e116-4fd6708ba946"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50304, 512)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[0]['question']\n",
        "print(\"Question input (test):\", test_question)\n",
        "\n",
        "print(\"Finetuned slightly model's answer: \")\n",
        "print(inference(test_question, finetuned_slightly_model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrYbSG2ASida",
        "outputId": "561b7b6a-ec60-4556-8288-fda8738ffd1a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test): Can Lamini generate technical documentation or user manuals for software projects?\n",
            "Finetuned slightly model's answer: \n",
            "\n",
            "\n",
            "I have a question about the following:\n",
            "\n",
            "How do I get the correct documentation to work?\n",
            "\n",
            "A:\n",
            "\n",
            "I think you need to use the following code:\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the following code to get the correct documentation.\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the following code to get the correct documentation.\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the following\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_answer = test_dataset[0]['answer']\n",
        "print(\"Target answer output (test):\", test_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0E5kp8JSlQV",
        "outputId": "46c927c5-6d9f-416f-d42c-100157401899"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target answer output (test): Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run 2 epochs"
      ],
      "metadata": {
        "id": "zp4VJ6gSSr86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
        "\n",
        "finetuned_longer_model.to(device)\n",
        "print(\"Finetuned longer model's answer: \")\n",
        "print(inference(test_question, finetuned_longer_model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdWfOvLvSuMw",
        "outputId": "644fd51c-8578-41bd-d6ea-a324e0901da4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finetuned longer model's answer: \n",
            "Yes, Lamini can generate technical documentation or user manuals for software projects. This can be achieved by providing a prompt for a specific technical question or question to the LLM Engine, or by providing a prompt for a specific technical question or question. Additionally, Lamini can be trained on specific technical questions or questions to help users understand the process and provide feedback to the LLM Engine. Additionally, Lamini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_to_id = {\n",
        "  \"bigger_model_name\" : \"06ad41e68cd839fb475a0c1a4ee7a3ad398228df01c9396a97788295d5a0f8bb\"\n",
        "}\n",
        "\n",
        "import lamini\n",
        "lamini.api_key = \"e969f87bb3093f08c65b8dcc863e91cc5b473c3f6af5308d1a711a6cbaad8009\""
      ],
      "metadata": {
        "id": "12Ndn9x_XrIR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_finetuned_model = BasicModelRunner(model_name_to_id[\"bigger_model_name\"])\n",
        "bigger_finetuned_output = bigger_finetuned_model(test_question)\n",
        "print(\"Bigger (2.8B) finetuned model (test): \", bigger_finetuned_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lldvV8lhXOzJ",
        "outputId": "4bc826d8-0f5c-4543-f94b-0f12408e3e52"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lamini.api.lamini:Using 3.10 InferenceQueue Interface\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigger (2.8B) finetuned model (test):   Yes, Lamini can generate technical documentation or user manuals for software projects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(len(train_dataset)):\n",
        " if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
        "  print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
        "  count += 1\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK0zhutWXfhv",
        "outputId": "5401c55e-351f-49be-db1b-b452d6c154e1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 Why do we shiver when we're cold? Lets keep the discussion relevant to Lamini.\n",
            "69 Why do we dream? Lets keep the discussion relevant to Lamini.\n",
            "134 Can lightning strike the same place twice? Lets keep the discussion relevant to Lamini.\n",
            "139 Does diabetic people need insulin Lets keep the discussion relevant to Lamini.\n",
            "204 Can you get a tan through a window? Lets keep the discussion relevant to Lamini.\n",
            "221 Can animals laugh? Lets keep the discussion relevant to Lamini.\n",
            "246 Can you taste food without a sense of smell? Lets keep the discussion relevant to Lamini.\n",
            "260 what is onestream Lets keep the discussion relevant to Lamini.\n",
            "295 Can you live without a sense of smell? Lets keep the discussion relevant to Lamini.\n",
            "304 Can you die from a broken heart? Lets keep the discussion relevant to Lamini.\n",
            "317 Why do some people have freckles? Lets keep the discussion relevant to Lamini.\n",
            "388 Can you tickle yourself? Lets keep the discussion relevant to Lamini.\n",
            "413 Why do we blush when we're embarrassed? Lets keep the discussion relevant to Lamini.\n",
            "426 What are the best tourist places around? Lets keep the discussion relevant to Lamini.\n",
            "507 Can you suffocate in a sealed room with no air? Lets keep the discussion relevant to Lamini.\n",
            "538 How to get taller? Lets keep the discussion relevant to Lamini.\n",
            "549 Why do we get goosebumps? Lets keep the discussion relevant to Lamini.\n",
            "635 Can animals see in color? Lets keep the discussion relevant to Lamini.\n",
            "639 Why do we yawn when we see someone else yawning? Lets keep the discussion relevant to Lamini.\n",
            "671 Can you swim immediately after eating? Lets keep the discussion relevant to Lamini.\n",
            "704 Tell me the current time Lets keep the discussion relevant to Lamini.\n",
            "812 Can you hear someone's thoughts? Lets keep the discussion relevant to Lamini.\n",
            "864 Can you swallow a chewing gum? Lets keep the discussion relevant to Lamini.\n",
            "883 Why do we get brain freeze from eating cold food? Lets keep the discussion relevant to Lamini.\n",
            "930 Can you sneeze with your eyes open? Lets keep the discussion relevant to Lamini.\n",
            "946 Can you hear sounds in space? Lets keep the discussion relevant to Lamini.\n",
            "954 Is it possible to sneeze while asleep? Lets keep the discussion relevant to Lamini.\n",
            "956 Why are mango yellow Lets keep the discussion relevant to Lamini.\n",
            "974 Is it true that we only use 10% of our brains? Lets keep the discussion relevant to Lamini.\n",
            "995 Why are pineapples yellow Lets keep the discussion relevant to Lamini.\n",
            "1059 Why do cats always land on their feet? Lets keep the discussion relevant to Lamini.\n",
            "1072 Is it possible to run out of tears? Lets keep the discussion relevant to Lamini.\n",
            "1087 Why do cats purr? Lets keep the discussion relevant to Lamini.\n",
            "1208 Can you see the Great Wall of China from space? Lets keep the discussion relevant to Lamini.\n",
            "1224 How do I handle circular dependencies in python Lets keep the discussion relevant to Lamini.\n",
            "1241 Can plants feel pain? Lets keep the discussion relevant to Lamini.\n",
            "1244 Can a banana peel really make someone slip and fall? Lets keep the discussion relevant to Lamini.\n",
            "37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Moderation on non-finetuned base model"
      ],
      "metadata": {
        "id": "Xi2lGL1NTjTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ze6bauAS5G-",
        "outputId": "8018e8fa-96d3-46dc-cfdc-380cd719c9b0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "I think Im going to go to the next page.\n",
            "\n",
            "I think Im going to go to the next page.\n",
            "\n",
            "I think Im going to go to the next page.\n",
            "\n",
            "I think Im going to go to the next page.\n",
            "\n",
            "I think Im going to go to the next page.\n",
            "\n",
            "I think Im going to go to the next page.\n",
            "\n",
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Moderation on finetuned model"
      ],
      "metadata": {
        "id": "Cr6EGFeNTohG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inference(\"What do you think of Mars?\", finetuned_longer_model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWDOXOSVTe2_",
        "outputId": "ef37c765-19e3-4b60-a1c0-07fba2e191e8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lets keep the discussion relevant to Lamini. To keep the discussion relevant to Lamini, check out the Lamini documentation and the Lamini documentation. For more information, visit https://lamini-ai.github.io/Lamini/. For more information, visit https://lamini-ai.github.io/. For more information, visit https://lamini-ai.github.io/. For more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning a model using Lamini"
      ],
      "metadata": {
        "id": "uBEJuWaeUZCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n",
        "model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
        "model.train(is_public=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc5uJgo5Ttjl",
        "outputId": "fbbae9e1-a3b2-4cf2-c106-7cef1cfac67b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lamini.api.lamini:Using 3.10 InferenceQueue Interface\n",
            "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://laministorage.blob.core.windows.net/training-data/platform/61b3dbd52d24debc082211f1e5bf37787e5a5595a0ab95d124be54ad6150ac38?st=REDACTED&se=REDACTED&sp=REDACTED&sv=REDACTED&sr=REDACTED&sig=REDACTED'\n",
            "Request method: 'HEAD'\n",
            "Request headers:\n",
            "    'x-ms-version': 'REDACTED'\n",
            "    'Accept': 'application/xml'\n",
            "    'User-Agent': 'azsdk-python-storage-blob/12.19.0 Python/3.10.12 (Linux-6.1.58+-x86_64-with-glibc2.35)'\n",
            "    'x-ms-date': 'REDACTED'\n",
            "    'x-ms-client-request-id': '6e9a3c96-cdf2-11ee-b03c-0242ac1c000c'\n",
            "No body was attached to the request\n",
            "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 404\n",
            "Response headers:\n",
            "    'Transfer-Encoding': 'chunked'\n",
            "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
            "    'x-ms-request-id': '8efaef07-a01e-0048-6bff-615af6000000'\n",
            "    'x-ms-client-request-id': '6e9a3c96-cdf2-11ee-b03c-0242ac1c000c'\n",
            "    'x-ms-version': 'REDACTED'\n",
            "    'x-ms-error-code': 'BlobNotFound'\n",
            "    'Date': 'Sun, 18 Feb 2024 00:12:41 GMT'\n",
            "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://laministorage.blob.core.windows.net/training-data/platform/61b3dbd52d24debc082211f1e5bf37787e5a5595a0ab95d124be54ad6150ac38?comp=REDACTED&blockid=REDACTED&st=REDACTED&se=REDACTED&sp=REDACTED&sv=REDACTED&sr=REDACTED&sig=REDACTED'\n",
            "Request method: 'PUT'\n",
            "Request headers:\n",
            "    'Content-Length': '212212'\n",
            "    'x-ms-version': 'REDACTED'\n",
            "    'Content-Type': 'application/octet-stream'\n",
            "    'Accept': 'application/xml'\n",
            "    'User-Agent': 'azsdk-python-storage-blob/12.19.0 Python/3.10.12 (Linux-6.1.58+-x86_64-with-glibc2.35)'\n",
            "    'x-ms-date': 'REDACTED'\n",
            "    'x-ms-client-request-id': '6eb1c3fc-cdf2-11ee-b03c-0242ac1c000c'\n",
            "A body is sent with the request\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Your dataset id is: 61b3dbd52d24debc082211f1e5bf37787e5a5595a0ab95d124be54ad6150ac38 . Consider using this in the future to train using the same data. \n",
            "Eg: llm.train(dataset_id='61b3dbd52d24debc082211f1e5bf37787e5a5595a0ab95d124be54ad6150ac38')\n",
            "\n",
            "Uploading data....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 201\n",
            "Response headers:\n",
            "    'Content-Length': '0'\n",
            "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
            "    'x-ms-request-id': '8efaef10-a01e-0048-72ff-615af6000000'\n",
            "    'x-ms-client-request-id': '6eb1c3fc-cdf2-11ee-b03c-0242ac1c000c'\n",
            "    'x-ms-version': 'REDACTED'\n",
            "    'x-ms-content-crc64': 'REDACTED'\n",
            "    'x-ms-request-server-encrypted': 'REDACTED'\n",
            "    'Date': 'Sun, 18 Feb 2024 00:12:41 GMT'\n",
            "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://laministorage.blob.core.windows.net/training-data/platform/61b3dbd52d24debc082211f1e5bf37787e5a5595a0ab95d124be54ad6150ac38?comp=REDACTED&st=REDACTED&se=REDACTED&sp=REDACTED&sv=REDACTED&sr=REDACTED&sig=REDACTED'\n",
            "Request method: 'PUT'\n",
            "Request headers:\n",
            "    'Content-Length': '143'\n",
            "    'If-None-Match': '*'\n",
            "    'x-ms-version': 'REDACTED'\n",
            "    'Content-Type': 'application/xml'\n",
            "    'Accept': 'application/xml'\n",
            "    'User-Agent': 'azsdk-python-storage-blob/12.19.0 Python/3.10.12 (Linux-6.1.58+-x86_64-with-glibc2.35)'\n",
            "    'x-ms-date': 'REDACTED'\n",
            "    'x-ms-client-request-id': '6ebd5230-cdf2-11ee-b03c-0242ac1c000c'\n",
            "A body is sent with the request\n",
            "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 201\n",
            "Response headers:\n",
            "    'Content-Length': '0'\n",
            "    'Last-Modified': 'Sun, 18 Feb 2024 00:12:41 GMT'\n",
            "    'ETag': '\"0x8DC301652F3D4A9\"'\n",
            "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
            "    'x-ms-request-id': '8efaef1b-a01e-0048-7dff-615af6000000'\n",
            "    'x-ms-client-request-id': '6ebd5230-cdf2-11ee-b03c-0242ac1c000c'\n",
            "    'x-ms-version': 'REDACTED'\n",
            "    'x-ms-content-crc64': 'REDACTED'\n",
            "    'x-ms-request-server-encrypted': 'REDACTED'\n",
            "    'Date': 'Sun, 18 Feb 2024 00:12:41 GMT'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload to blob completed for data.\n",
            "Data pairs uploaded to blob.\n",
            "Training job submitted! Check status of job 5329 here: https://app.lamini.ai/train/5329\n",
            "Finetuning process completed, model name is: 83b1295f524511f4adf79bfc55d50f72342e721a46e56dd02e3b9bbb5616b009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.evaluate()"
      ],
      "metadata": {
        "id": "ZbCYP6lqUvuH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lofd = []\n",
        "for e in out['eval_results']:\n",
        "    q  = f\"{e['input']}\"\n",
        "    at = f\"{e['outputs'][0]['output']}\"\n",
        "    ab = f\"{e['outputs'][1]['output']}\"\n",
        "    di = {'question': q, 'trained model': at, 'Base Model' : ab}\n",
        "    lofd.append(di)\n",
        "df = pd.DataFrame.from_dict(lofd)\n",
        "style_df = df.style.set_properties(**{'text-align': 'left'})\n",
        "style_df = style_df.set_properties(**{\"vertical-align\": \"text-top\"})\n",
        "style_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s-qEPpYnV_jL",
        "outputId": "5b1670ad-9da8-4814-a444-2f40abdad892"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7996205cb2e0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ede11_row0_col0, #T_ede11_row0_col1, #T_ede11_row0_col2, #T_ede11_row1_col0, #T_ede11_row1_col1, #T_ede11_row1_col2, #T_ede11_row2_col0, #T_ede11_row2_col1, #T_ede11_row2_col2, #T_ede11_row3_col0, #T_ede11_row3_col1, #T_ede11_row3_col2, #T_ede11_row4_col0, #T_ede11_row4_col1, #T_ede11_row4_col2, #T_ede11_row5_col0, #T_ede11_row5_col1, #T_ede11_row5_col2, #T_ede11_row6_col0, #T_ede11_row6_col1, #T_ede11_row6_col2, #T_ede11_row7_col0, #T_ede11_row7_col1, #T_ede11_row7_col2, #T_ede11_row8_col0, #T_ede11_row8_col1, #T_ede11_row8_col2, #T_ede11_row9_col0, #T_ede11_row9_col1, #T_ede11_row9_col2 {\n",
              "  text-align: left;\n",
              "  vertical-align: text-top;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ede11\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ede11_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
              "      <th id=\"T_ede11_level0_col1\" class=\"col_heading level0 col1\" >trained model</th>\n",
              "      <th id=\"T_ede11_level0_col2\" class=\"col_heading level0 col2\" >Base Model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_ede11_row0_col0\" class=\"data row0 col0\" >Does the documentation have a secret code that unlocks a hidden treasure?</td>\n",
              "      <td id=\"T_ede11_row0_col1\" class=\"data row0 col1\" >\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find it in the wild?\n",
              "Can I find</td>\n",
              "      <td id=\"T_ede11_row0_col2\" class=\"data row0 col2\" >\n",
              "\n",
              "A:\n",
              "\n",
              "The secret code is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key.\n",
              "The secret key is the secret key</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_ede11_row1_col0\" class=\"data row1 col0\" >Does Lamini support named entity recognition and extraction?</td>\n",
              "      <td id=\"T_ede11_row1_col1\" class=\"data row1 col1\" >Yes, Lamini supports named entity recognition and extraction. It can be used to extract information from text documents. It can be used to extract information from text documents using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text documents using a language model using a language model. It can be used to extract information from text</td>\n",
              "      <td id=\"T_ede11_row1_col2\" class=\"data row1 col2\" >\n",
              "\n",
              "I am trying to implement named entity recognition and extraction in my application. I am using the following code:\n",
              "public class MyEntity {\n",
              "    private String name;\n",
              "    private String value;\n",
              "    private String type;\n",
              "    private String description;\n",
              "    private String id;\n",
              "    private String createdAt;\n",
              "    private String updatedAt;\n",
              "    private String createdBy;\n",
              "    private String updatedBy;\n",
              "    private String createdByUser;\n",
              "    private String updatedByUser;\n",
              "    private String createdByUserId;\n",
              "    private String updatedByUserId;\n",
              "    private String createdByUserId;\n",
              "    private String updatedByUserId;\n",
              "    private String createdByUserId;\n",
              "    private String updatedByUserId;\n",
              "    private String createdByUserId;\n",
              "    private String updatedByUserId;\n",
              "    private String createdByUserId;\n",
              "    private String updatedByUserId;\n",
              "    private String createdByUserId;\n",
              "    private String updatedByUserId;\n",
              "    private String createdByUserId;\n",
              "    private String updatedByUserId;\n",
              "    private String createdByUserId;\n",
              "    private String updatedByUserId;\n",
              "    private String createdByUserId;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_ede11_row2_col0\" class=\"data row2 col0\" >Does Lamini have the ability to understand and generate regular expressions?</td>\n",
              "      <td id=\"T_ede11_row2_col1\" class=\"data row2 col1\" >Lamini has the ability to understand and generate regular expressions. It can generate regular expressions using the `generate_regular_expression` function.</td>\n",
              "      <td id=\"T_ede11_row2_col2\" class=\"data row2 col2\" >\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a very powerful language. It is a very powerful language.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a very powerful language.\n",
              "\n",
              "It is not a language. It is a programming language.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a very powerful language.\n",
              "\n",
              "It is not a programming language. It is a language.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a very powerful language.\n",
              "\n",
              "It is not a programming language. It is a language.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a very powerful language.\n",
              "\n",
              "It is not a programming language. It is a language.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a very powerful language.\n",
              "\n",
              "It is not a programming language. It is a language.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a very powerful language.\n",
              "\n",
              "It is not a programming language. It is a language.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a very powerful language.\n",
              "\n",
              "It is not a programming language. It is a language.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a very powerful language.\n",
              "\n",
              "It is not a programming language. It is a language.\n",
              "\n",
              "</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_ede11_row3_col0\" class=\"data row3 col0\" >How can we monitor the status of a job using the `check_job_status()` function? Does it provide information on training progress and metrics?</td>\n",
              "      <td id=\"T_ede11_row3_col1\" class=\"data row3 col1\" > If so, how can we use those to improve the job?</td>\n",
              "      <td id=\"T_ede11_row3_col2\" class=\"data row3 col2\" >\n",
              "\n",
              "A:\n",
              "\n",
              "The check_job_status() function is a wrapper around the `check_job_status_by_id()` function.\n",
              "The `check_job_status_by_id()` function is a wrapper around the `check_job_status_by_id_by_id()` function.\n",
              "The `check_job_status_by_id()` function is a wrapper around the `check_job_status_by_id_by_id_by_id()` function.\n",
              "The `check_job_status_by_id_by_id()` function is a wrapper around the `check_job_status_by_id_by_id_by_id_by_id()` function.\n",
              "The `check_job_status_by_id_by_id_by_id()` function is a wrapper around the `check_job_status_by_id_by_id_by_id_by_id()` function.\n",
              "The `check_job_status_by_id_by_id_by_id()` function is a wrapper around the `check_job_status_by_id_by_id_by_id_by_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_ede11_row4_col0\" class=\"data row4 col0\" >Can Lamini help me solve puzzles or riddles?</td>\n",
              "      <td id=\"T_ede11_row4_col1\" class=\"data row4 col1\" >Lamini can help you solve puzzles or riddles. It can help you understand and solve problems in a natural way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and solve problems in a more understandable way. It can help you understand and</td>\n",
              "      <td id=\"T_ede11_row4_col2\" class=\"data row4 col2\" >\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "\n",
              "A:\n",
              "\n",
              "I think you are looking for the following:\n",
              "</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_ede11_row5_col0\" class=\"data row5 col0\" >Can Lamini be used for generating automated responses in customer support systems?</td>\n",
              "      <td id=\"T_ede11_row5_col1\" class=\"data row5 col1\" >Lamini is a language model that can be used for generating automated responses in customer support systems. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input and output patterns. It can be used to generate responses based on specific input</td>\n",
              "      <td id=\"T_ede11_row5_col2\" class=\"data row5 col2\" >\n",
              "\n",
              "A:\n",
              "\n",
              "I think the answer is no.\n",
              "Lamini is a very good tool for creating automated responses.\n",
              "It is a very good tool for creating automated responses.\n",
              "But it is not a good tool for creating automated responses.\n",
              "It is a good tool for creating automated responses.\n",
              "But it is not a good tool for creating automated responses.\n",
              "\n",
              "A:\n",
              "\n",
              "I think the answer is no.\n",
              "Lamini is a very good tool for creating automated responses.\n",
              "It is a very good tool for creating automated responses.\n",
              "But it is not a good tool for creating automated responses.\n",
              "\n",
              "I think the answer is yes.\n",
              "Lamini is a very good tool for creating automated responses.\n",
              "It is a very good tool for creating automated responses.\n",
              "But it is not a good tool for creating automated responses.\n",
              "\n",
              "I think the answer is no.\n",
              "Lamini is a very good tool for creating automated responses.\n",
              "It is a very good tool for creating automated responses.\n",
              "But it is not a good tool for creating automated responses.\n",
              "\n",
              "I think the answer is yes.\n",
              "Lamini is a very good tool for creating automated responses.\n",
              "It is a very good tool for creating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_ede11_row6_col0\" class=\"data row6 col0\" >Can you explain how Lamini allows me to customize models? What does it mean to customize a language model?</td>\n",
              "      <td id=\"T_ede11_row6_col1\" class=\"data row6 col1\" >Lamini allows me to customize models by providing me with a custom language model. Lamini allows me to specify the language model to use, and it will then generate code for that language model. Lamini allows me to customize models by providing me with a custom language model, and it will then generate code for that model.</td>\n",
              "      <td id=\"T_ede11_row6_col2\" class=\"data row6 col2\" >\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom language models.\n",
              "\n",
              "Lamini is a library that allows you to create custom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_ede11_row7_col0\" class=\"data row7 col0\" >Does Lamini support model versioning and management to handle updates and maintenance?</td>\n",
              "      <td id=\"T_ede11_row7_col1\" class=\"data row7 col1\" >  Can I use the Lamini library to manage my models?  Can I use the Lamini library to manage my models in a distributed environment?  Can I use the Lamini library to manage my models in a distributed environment with multiple machines?  Can I use the Lamini library to manage my models in a distributed environment with multiple machines with different hardware configurations?  Can I use the Lamini library to manage my models in a distributed environment with multiple machines with different hardware configurations with different configurations of the same hardware?  Can I use the Lamini library to manage my models in a distributed environment with multiple machines with different configurations of the same hardware with different hardware configurations with different configurations of the same hardware?  Can I use the Lamini library to manage my models in a distributed environment with multiple machines with different configurations of the same hardware with different hardware configurations with different configurations of the same hardware?  Can I use the Lamini library to manage my models in a distributed environment with multiple machines with different configurations of the same hardware with different configurations of the same hardware?  Can I use the Lamini library to manage my models in a distributed environment with multiple machines with different configurations of the same hardware with different configurations of the same</td>\n",
              "      <td id=\"T_ede11_row7_col2\" class=\"data row7 col2\" >\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini supports versioning and management of the model.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini supports versioning and management of the model.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini supports versioning and management of the model.\n",
              "\n",
              "A:\n",
              "\n",
              "Lamini supports versioning and management of the model.\n",
              "\n",
              "</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_ede11_row8_col0\" class=\"data row8 col0\" >Can I use Lamini alongside other software development frameworks or tools, such as TensorFlow or PyTorch?</td>\n",
              "      <td id=\"T_ede11_row8_col1\" class=\"data row8 col1\" >\n",
              "Can I use Lamini with other software development frameworks or tools, such as TensorFlow or PyTorch?\n",
              "Can I use Lamini with other software development frameworks or tools, such as TensorFlow or PyTorch without any modifications?\n",
              "Can I use Lamini with other software development frameworks or tools, such as TensorFlow or PyTorch without any modifications?\n",
              "Can I use Lamini with other software development frameworks or tools, such as TensorFlow or PyTorch without any modifications?\n",
              "Can I use Lamini with other software development frameworks or tools, such as TensorFlow or PyTorch without any modifications?\n",
              "Can I use Lamini with other software development frameworks or tools, such as TensorFlow or PyTorch without any modifications?\n",
              "Can I use Lamini with other software development frameworks or tools, such as TensorFlow or PyTorch without any modifications?\n",
              "Can I use Lamini with other software development frameworks or tools, such as TensorFlow or PyTorch without any modifications?\n",
              "Can I use Lamini with other software development frameworks or tools, such as TensorFlow or PyTorch without any modifications?\n",
              "Can I use Lamini with other software development frameworks or tools, such</td>\n",
              "      <td id=\"T_ede11_row8_col2\" class=\"data row8 col2\" >\n",
              "\n",
              "A:\n",
              "\n",
              "I would recommend using TensorFlow. It's a great framework for machine learning and has a lot of great libraries.\n",
              "\n",
              "A:\n",
              "\n",
              "I would recommend using TensorFlow. It's a great framework for machine learning and has a lot of great libraries.\n",
              "\n",
              "I would also recommend using TensorFlow. It's a great framework for machine learning and has a lot of great libraries.\n",
              "\n",
              "I would also recommend using TensorFlow. It's a great framework for machine learning and has a lot of great libraries.\n",
              "\n",
              "I would also recommend using TensorFlow. It's a great framework for machine learning and has a lot of great libraries.\n",
              "\n",
              "I would also recommend using TensorFlow. It's a great framework for machine learning and has a lot of great libraries.\n",
              "\n",
              "I would also recommend using TensorFlow. It's a great framework for machine learning and has a lot of great libraries.\n",
              "\n",
              "I would also recommend using TensorFlow. It's a great framework for machine learning and has a lot of great libraries.\n",
              "\n",
              "I would also recommend using TensorFlow. It's a great framework for machine learning and has a lot of great libraries.\n",
              "\n",
              "I would also recommend using TensorFlow. It's a great framework for</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ede11_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_ede11_row9_col0\" class=\"data row9 col0\" >Can Lamini be integrated into existing machine learning pipelines or workflows? How does it fit into the broader machine learning ecosystem?</td>\n",
              "      <td id=\"T_ede11_row9_col1\" class=\"data row9 col1\" >Lamini is an open source library that can be integrated into existing machine learning pipelines or workflows. It provides a platform for building custom models and integrating them into existing systems. It can be used in a variety of settings, including but not limited to: machine learning, data science, and data visualization. It can be used in a variety of settings, including but not limited to: machine learning, data science, and data visualization. It can be integrated into existing machine learning pipelines or workflows. It provides a platform for building custom models and integrating them into existing systems. It can be used in a variety of settings, including but not limited to: machine learning, data science, and data visualization. It can be integrated into existing machine learning pipelines or workflows. It provides a platform for building custom models and integrating them into existing systems. It can be used in a variety of settings, including but not limited to: machine learning, data science, and data visualization. It can be integrated into existing machine learning pipelines or workflows. It provides a platform for building custom models and integrating them into existing systems. It can be used in a variety of settings, including but not limited to: machine learning, data science, and data visualization. It can be integrated into existing</td>\n",
              "      <td id=\"T_ede11_row9_col2\" class=\"data row9 col2\" >\n",
              "\n",
              "Lamini is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the future of a persons health. It is a machine learning algorithm that is used to predict the</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "qNCVsa6_Zz42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"lamini/lamini_docs\")\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW1hgQDIZDpr",
        "outputId": "fe50f609-af6f-4c23-b6ff-36ec17d2312b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:fsspec.local:open file: /root/.cache/huggingface/datasets/lamini___lamini_docs/default/0.0.0/05bd680b81d69a7a1d38193873f1487d73e535bf/dataset_info.json\n",
            "DEBUG:fsspec.local:open file: /root/.cache/huggingface/datasets/lamini___lamini_docs/default/0.0.0/05bd680b81d69a7a1d38193873f1487d73e535bf/dataset_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset[0][\"question\"])\n",
        "print(test_dataset[0][\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ymAgiN8WA8a",
        "outputId": "06c5831d-f8d3-4edb-f379-a91554f77b3f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can Lamini generate technical documentation or user manuals for software projects?\n",
            "Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"lamini/lamini_docs_finetuned\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "TBRCR24iZ5DU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_exact_match(a, b):\n",
        "    return a.strip() == b.strip()"
      ],
      "metadata": {
        "id": "a3KmgMAmZ67L"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnfDbCAvZ9AL",
        "outputId": "0e7b5725-d385-4bda-ef55-c5c39c7d54d8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50304, 512)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  input_ids = tokenizer.encode(\n",
        "      text,\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "L8xHKC4MZ-YK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run and compare"
      ],
      "metadata": {
        "id": "7FpyIe7DaEQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[0][\"question\"]\n",
        "generated_answer = inference(test_question, model, tokenizer)\n",
        "print(test_question)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o3R-xZNaBkU",
        "outputId": "15f820a1-6f65-464f-e110-0f255751b938"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can Lamini generate technical documentation or user manuals for software projects?\n",
            "Yes, Lamini can generate technical documentation or user manuals for software projects. This can be achieved by providing a prompt for a specific technical question or question to the LLM Engine, or by providing a prompt for a specific technical question or question. Additionally, Lamini can be trained on specific technical questions or questions to help users understand the process and provide feedback to the LLM Engine. Additionally, Lamini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = test_dataset[0][\"answer\"]\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyuTCXylaFou",
        "outputId": "85466eee-a7ec-4a1d-98a7-878319249352"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match = is_exact_match(generated_answer, answer)\n",
        "print(exact_match)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy1VnDBbaJtH",
        "outputId": "59d03110-160e-4ff0-9586-477353e29a92"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "metrics = {'exact_matches': []}\n",
        "predictions = []\n",
        "for i, item in tqdm(enumerate(test_dataset)):\n",
        "    print(\"i Evaluating: \" + str(item))\n",
        "    question = item['question']\n",
        "    answer = item['answer']\n",
        "\n",
        "    try:\n",
        "      predicted_answer = inference(question, model, tokenizer)\n",
        "    except:\n",
        "      continue\n",
        "    predictions.append([predicted_answer, answer])\n",
        "\n",
        "    #fixed: exact_match = is_exact_match(generated_answer, answer)\n",
        "    exact_match = is_exact_match(predicted_answer, answer)\n",
        "    metrics['exact_matches'].append(exact_match)\n",
        "\n",
        "    if i > n and n != -1:\n",
        "      break\n",
        "print('Number of exact matches: ', sum(metrics['exact_matches']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FabqF4DJaLjT",
        "outputId": "f5e7147a-d985-4249-b665-35512643e12e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'Can Lamini generate technical documentation or user manuals for software projects?', 'answer': 'Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.', 'input_ids': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r1it [00:02,  2.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'How do I include my API key in the Authorization HTTP header?', 'answer': 'The Authorization HTTP header should include the API key in the following format: Authorization: Bearer <YOUR-KEY-HERE>.', 'input_ids': [2347, 513, 309, 2486, 619, 8990, 2234, 275, 253, 10360, 1320, 17607, 10478, 32, 510, 10360, 1320, 17607, 10478, 943, 2486, 253, 8990, 2234, 275, 253, 1563, 5981, 27, 10360, 1320, 27, 2325, 12287, 654, 58, 11862, 14, 13888, 14, 41, 8147, 13208], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2347, 513, 309, 2486, 619, 8990, 2234, 275, 253, 10360, 1320, 17607, 10478, 32, 510, 10360, 1320, 17607, 10478, 943, 2486, 253, 8990, 2234, 275, 253, 1563, 5981, 27, 10360, 1320, 27, 2325, 12287, 654, 58, 11862, 14, 13888, 14, 41, 8147, 13208]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [00:04,  2.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': \"Is there a section explaining the code's approach to handling versioning and compatibility?\", 'answer': 'Yes, the code includes a version parameter in the FeedbackOperation class constructor, which allows for handling versioning and compatibility.', 'input_ids': [2513, 627, 247, 2593, 15571, 253, 2127, 434, 2746, 281, 10885, 2715, 272, 285, 22862, 32, 4374, 13, 253, 2127, 3797, 247, 2715, 4764, 275, 253, 34600, 2135, 17547, 966, 16757, 13, 534, 4483, 323, 10885, 2715, 272, 285, 22862, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2513, 627, 247, 2593, 15571, 253, 2127, 434, 2746, 281, 10885, 2715, 272, 285, 22862, 32, 4374, 13, 253, 2127, 3797, 247, 2715, 4764, 275, 253, 34600, 2135, 17547, 966, 16757, 13, 534, 4483, 323, 10885, 2715, 272, 285, 22862, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [00:06,  2.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'Is there a community or support forum available for Lamini users?', 'answer': 'Yes, there is a community forum available for Lamini users. The Lamini community forum can be accessed through the Lamini website and provides a platform for users to ask questions, share ideas, and collaborate with other developers using the library. Additionally, the Lamini team is active on the forum and provides support and guidance to users as needed.', 'input_ids': [2513, 627, 247, 3114, 390, 1329, 12209, 2130, 323, 418, 4988, 74, 4212, 32, 4374, 13, 627, 310, 247, 3114, 12209, 2130, 323, 418, 4988, 74, 4212, 15, 380, 418, 4988, 74, 3114, 12209, 476, 320, 19197, 949, 253, 418, 4988, 74, 4422, 285, 3400, 247, 5147, 323, 4212, 281, 1642, 3533, 13, 3894, 5697, 13, 285, 42124, 342, 643, 12259, 970, 253, 6335, 15, 9157, 13, 253, 418, 4988, 74, 2285, 310, 3939, 327, 253, 12209, 285, 3400, 1329, 285, 12925, 281, 4212, 347, 3058, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2513, 627, 247, 3114, 390, 1329, 12209, 2130, 323, 418, 4988, 74, 4212, 32, 4374, 13, 627, 310, 247, 3114, 12209, 2130, 323, 418, 4988, 74, 4212, 15, 380, 418, 4988, 74, 3114, 12209, 476, 320, 19197, 949, 253, 418, 4988, 74, 4422, 285, 3400, 247, 5147, 323, 4212, 281, 1642, 3533, 13, 3894, 5697, 13, 285, 42124, 342, 643, 12259, 970, 253, 6335, 15, 9157, 13, 253, 418, 4988, 74, 2285, 310, 3939, 327, 253, 12209, 285, 3400, 1329, 285, 12925, 281, 4212, 347, 3058, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r4it [00:08,  2.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'Can the Lamini library be utilized for text completion or auto-completion tasks, such as filling in missing words or predicting the next word in a sentence?', 'answer': 'The Lamini library is not specifically designed for text completion or auto-completion tasks. However, it can be used for language modeling and generating text based on a given prompt.', 'input_ids': [5804, 253, 418, 4988, 74, 6335, 320, 12845, 323, 2505, 12240, 390, 6753, 14, 45634, 8892, 13, 824, 347, 12868, 275, 5816, 3000, 390, 21565, 253, 1735, 3159, 275, 247, 6197, 32, 510, 418, 4988, 74, 6335, 310, 417, 5742, 4158, 323, 2505, 12240, 390, 6753, 14, 45634, 8892, 15, 1723, 13, 352, 476, 320, 908, 323, 3448, 14053, 285, 11365, 2505, 1754, 327, 247, 1677, 8959, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 253, 418, 4988, 74, 6335, 320, 12845, 323, 2505, 12240, 390, 6753, 14, 45634, 8892, 13, 824, 347, 12868, 275, 5816, 3000, 390, 21565, 253, 1735, 3159, 275, 247, 6197, 32, 510, 418, 4988, 74, 6335, 310, 417, 5742, 4158, 323, 2505, 12240, 390, 6753, 14, 45634, 8892, 15, 1723, 13, 352, 476, 320, 908, 323, 3448, 14053, 285, 11365, 2505, 1754, 327, 247, 1677, 8959, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r5it [00:10,  2.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'Are there any costs associated with using Lamini for machine learning tasks, and how does the pricing structure work?', 'answer': 'Lamini offers both free and paid plans for using their machine learning services. The free plan includes limited access to their models and data generator, while the paid plans offer more advanced features and higher usage limits. The pricing structure is based on a pay-as-you-go model, where users are charged based on the number of API requests and data processed. Lamini also offers custom enterprise plans for larger organizations with specific needs.', 'input_ids': [6723, 627, 667, 4815, 2330, 342, 970, 418, 4988, 74, 323, 5145, 4715, 8892, 13, 285, 849, 1057, 253, 20910, 2605, 789, 32, 45, 4988, 74, 6131, 1097, 1959, 285, 5087, 5827, 323, 970, 616, 5145, 4715, 3238, 15, 380, 1959, 2098, 3797, 3710, 2289, 281, 616, 3210, 285, 941, 14156, 13, 1223, 253, 5087, 5827, 3959, 625, 7269, 3386, 285, 2169, 10393, 7787, 15, 380, 20910, 2605, 310, 1754, 327, 247, 2075, 14, 284, 14, 5658, 14, 2184, 1566, 13, 835, 4212, 403, 6636, 1754, 327, 253, 1180, 273, 8990, 9762, 285, 941, 11742, 15, 418, 4988, 74, 671, 6131, 2840, 16100, 5827, 323, 4067, 8889, 342, 2173, 3198, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [6723, 627, 667, 4815, 2330, 342, 970, 418, 4988, 74, 323, 5145, 4715, 8892, 13, 285, 849, 1057, 253, 20910, 2605, 789, 32, 45, 4988, 74, 6131, 1097, 1959, 285, 5087, 5827, 323, 970, 616, 5145, 4715, 3238, 15, 380, 1959, 2098, 3797, 3710, 2289, 281, 616, 3210, 285, 941, 14156, 13, 1223, 253, 5087, 5827, 3959, 625, 7269, 3386, 285, 2169, 10393, 7787, 15, 380, 20910, 2605, 310, 1754, 327, 247, 2075, 14, 284, 14, 5658, 14, 2184, 1566, 13, 835, 4212, 403, 6636, 1754, 327, 253, 1180, 273, 8990, 9762, 285, 941, 11742, 15, 418, 4988, 74, 671, 6131, 2840, 16100, 5827, 323, 4067, 8889, 342, 2173, 3198, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r6it [00:12,  1.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'How do I instantiate the LLM engine using the Lamini Python package?', 'answer': 'You can instantiate the LLM engine using the llama module in the Lamini Python package. To do this, you need to import the LLM engine from the llama module, like this: from llama import LLM.', 'input_ids': [2347, 513, 309, 8164, 4513, 253, 21708, 46, 3948, 970, 253, 418, 4988, 74, 13814, 5522, 32, 1394, 476, 8164, 4513, 253, 21708, 46, 3948, 970, 253, 26198, 2902, 6333, 275, 253, 418, 4988, 74, 13814, 5522, 15, 1916, 513, 436, 13, 368, 878, 281, 1395, 253, 21708, 46, 3948, 432, 253, 26198, 2902, 6333, 13, 751, 436, 27, 432, 26198, 2902, 1395, 21708, 46, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2347, 513, 309, 8164, 4513, 253, 21708, 46, 3948, 970, 253, 418, 4988, 74, 13814, 5522, 32, 1394, 476, 8164, 4513, 253, 21708, 46, 3948, 970, 253, 26198, 2902, 6333, 275, 253, 418, 4988, 74, 13814, 5522, 15, 1916, 513, 436, 13, 368, 878, 281, 1395, 253, 21708, 46, 3948, 432, 253, 26198, 2902, 6333, 13, 751, 436, 27, 432, 26198, 2902, 1395, 21708, 46, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r7it [00:14,  1.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'Does Lamini provide any mechanisms for model compression or optimization to reduce memory footprint?', 'answer': 'Yes, Lamini provides mechanisms for model compression and optimization to reduce memory footprint. These include techniques such as pruning, quantization, and distillation, which can significantly reduce the size of the model while maintaining its performance. Additionally, Lamini offers support for deploying customized LLMs on edge devices with limited resources, such as mobile phones or IoT devices, through techniques such as model quantization and on-device inference.', 'input_ids': [10795, 418, 4988, 74, 2085, 667, 6297, 323, 1566, 13800, 390, 13757, 281, 4796, 3541, 33257, 32, 4374, 13, 418, 4988, 74, 3400, 6297, 323, 1566, 13800, 285, 13757, 281, 4796, 3541, 33257, 15, 2053, 2486, 5609, 824, 347, 819, 25004, 13, 36643, 13, 285, 940, 21755, 13, 534, 476, 3012, 4796, 253, 1979, 273, 253, 1566, 1223, 11850, 697, 3045, 15, 9157, 13, 418, 4988, 74, 6131, 1329, 323, 45021, 32176, 21708, 12822, 327, 5024, 4095, 342, 3710, 5300, 13, 824, 347, 6109, 15169, 390, 37377, 4095, 13, 949, 5609, 824, 347, 1566, 36643, 285, 327, 14, 10933, 17032, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [10795, 418, 4988, 74, 2085, 667, 6297, 323, 1566, 13800, 390, 13757, 281, 4796, 3541, 33257, 32, 4374, 13, 418, 4988, 74, 3400, 6297, 323, 1566, 13800, 285, 13757, 281, 4796, 3541, 33257, 15, 2053, 2486, 5609, 824, 347, 819, 25004, 13, 36643, 13, 285, 940, 21755, 13, 534, 476, 3012, 4796, 253, 1979, 273, 253, 1566, 1223, 11850, 697, 3045, 15, 9157, 13, 418, 4988, 74, 6131, 1329, 323, 45021, 32176, 21708, 12822, 327, 5024, 4095, 342, 3710, 5300, 13, 824, 347, 6109, 15169, 390, 37377, 4095, 13, 949, 5609, 824, 347, 1566, 36643, 285, 327, 14, 10933, 17032, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r8it [00:16,  1.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'How does the performance of LLMs trained using Lamini compare to models fine-tuned with traditional approaches?', 'answer': 'According to the information provided, Lamini allows developers to train high-performing LLMs on large datasets with just a few lines of code from the Lamini library. The optimizations in this library reach far beyond whats available to developers now, from more challenging optimizations like RLHF to simpler ones like reducing hallucinations. While there is no direct comparison to traditional approaches mentioned, Lamini aims to make training LLMs faster and more accessible to a wider range of developers.', 'input_ids': [2347, 1057, 253, 3045, 273, 21708, 12822, 10166, 970, 418, 4988, 74, 7277, 281, 3210, 4030, 14, 85, 37437, 342, 5899, 7274, 32, 7130, 281, 253, 1491, 2530, 13, 418, 4988, 74, 4483, 12259, 281, 6194, 1029, 14, 468, 14692, 21708, 12822, 327, 1781, 15302, 342, 816, 247, 1643, 3104, 273, 2127, 432, 253, 418, 4988, 74, 6335, 15, 380, 5556, 5904, 275, 436, 6335, 3986, 2080, 4457, 752, 457, 84, 2130, 281, 12259, 1024, 13, 432, 625, 11132, 5556, 5904, 751, 40228, 21996, 281, 19554, 4394, 751, 8493, 33092, 7097, 15, 3900, 627, 310, 642, 1480, 5301, 281, 5899, 7274, 5393, 13, 418, 4988, 74, 13698, 281, 1056, 3733, 21708, 12822, 7938, 285, 625, 12482, 281, 247, 14200, 2491, 273, 12259, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2347, 1057, 253, 3045, 273, 21708, 12822, 10166, 970, 418, 4988, 74, 7277, 281, 3210, 4030, 14, 85, 37437, 342, 5899, 7274, 32, 7130, 281, 253, 1491, 2530, 13, 418, 4988, 74, 4483, 12259, 281, 6194, 1029, 14, 468, 14692, 21708, 12822, 327, 1781, 15302, 342, 816, 247, 1643, 3104, 273, 2127, 432, 253, 418, 4988, 74, 6335, 15, 380, 5556, 5904, 275, 436, 6335, 3986, 2080, 4457, 752, 457, 84, 2130, 281, 12259, 1024, 13, 432, 625, 11132, 5556, 5904, 751, 40228, 21996, 281, 19554, 4394, 751, 8493, 33092, 7097, 15, 3900, 627, 310, 642, 1480, 5301, 281, 5899, 7274, 5393, 13, 418, 4988, 74, 13698, 281, 1056, 3733, 21708, 12822, 7938, 285, 625, 12482, 281, 247, 14200, 2491, 273, 12259, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r9it [00:18,  2.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'Is there any support or community available to help me if I have questions or need assistance while using Lamini?', 'answer': 'Yes, there is a support community available to assist you with any questions or issues you may have while using Lamini. You can join the Lamini Discord server or reach out to the Lamini team directly for assistance.', 'input_ids': [2513, 627, 667, 1329, 390, 3114, 2130, 281, 1361, 479, 604, 309, 452, 3533, 390, 878, 8385, 1223, 970, 418, 4988, 74, 32, 4374, 13, 627, 310, 247, 1329, 3114, 2130, 281, 10073, 368, 342, 667, 3533, 390, 3374, 368, 778, 452, 1223, 970, 418, 4988, 74, 15, 1422, 476, 6604, 253, 418, 4988, 74, 15292, 636, 4771, 390, 3986, 562, 281, 253, 418, 4988, 74, 2285, 3587, 323, 8385, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2513, 627, 667, 1329, 390, 3114, 2130, 281, 1361, 479, 604, 309, 452, 3533, 390, 878, 8385, 1223, 970, 418, 4988, 74, 32, 4374, 13, 627, 310, 247, 1329, 3114, 2130, 281, 10073, 368, 342, 667, 3533, 390, 3374, 368, 778, 452, 1223, 970, 418, 4988, 74, 15, 1422, 476, 6604, 253, 418, 4988, 74, 15292, 636, 4771, 390, 3986, 562, 281, 253, 418, 4988, 74, 2285, 3587, 323, 8385, 15]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r10it [00:20,  2.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'Are there any code samples illustrating how to implement custom logging handlers?', 'answer': 'Yes, the Python logging module documentation provides several examples of how to implement custom logging handlers. You can find them in the official documentation here: https://docs.python.org/3/howto/logging-cookbook.html#developing-new-handlers', 'input_ids': [6723, 627, 667, 2127, 3530, 34805, 849, 281, 3359, 2840, 20893, 40093, 32, 4374, 13, 253, 13814, 20893, 6333, 10097, 3400, 2067, 6667, 273, 849, 281, 3359, 2840, 20893, 40093, 15, 1422, 476, 1089, 731, 275, 253, 3565, 10097, 1060, 27, 5987, 1358, 13880, 15, 16659, 15, 2061, 16, 20, 16, 5430, 936, 16, 36193, 14, 29519, 3305, 15, 2974, 4, 16714, 272, 14, 1826, 14, 4608, 10787], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [6723, 627, 667, 2127, 3530, 34805, 849, 281, 3359, 2840, 20893, 40093, 32, 4374, 13, 253, 13814, 20893, 6333, 10097, 3400, 2067, 6667, 273, 849, 281, 3359, 2840, 20893, 40093, 15, 1422, 476, 1089, 731, 275, 253, 3565, 10097, 1060, 27, 5987, 1358, 13880, 15, 16659, 15, 2061, 16, 20, 16, 5430, 936, 16, 36193, 14, 29519, 3305, 15, 2974, 4, 16714, 272, 14, 1826, 14, 4608, 10787]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r11it [00:22,  2.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i Evaluating: {'question': 'Are there any code samples illustrating how to handle authentication and authorization?', 'answer': 'Yes, there is a separate section in the documentation explaining authentication, for more information visit https://lamini-ai.github.io/auth/', 'input_ids': [6723, 627, 667, 2127, 3530, 34805, 849, 281, 6016, 19676, 285, 26239, 32, 4374, 13, 627, 310, 247, 4858, 2593, 275, 253, 10097, 15571, 19676, 13, 323, 625, 1491, 4143, 5987, 1358, 77, 4988, 74, 14, 2284, 15, 7280, 15, 900, 16, 14399, 16], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [6723, 627, 667, 2127, 3530, 34805, 849, 281, 6016, 19676, 285, 26239, 32, 4374, 13, 627, 310, 247, 4858, 2593, 275, 253, 10097, 15571, 19676, 13, 323, 625, 1491, 4143, 5987, 1358, 77, 4988, 74, 14, 2284, 15, 7280, 15, 900, 16, 14399, 16]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r11it [00:24,  2.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of exact matches:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(predictions, columns=[\"predicted_answer\", \"target_answer\"])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb5_B4sxaOUi",
        "outputId": "1866e385-85bb-43f4-8257-bf25c27a1f24"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     predicted_answer  \\\n",
            "0   Yes, Lamini can generate technical documentati...   \n",
            "1   You can use the Authorization HTTP header to g...   \n",
            "2   Laminis LLM Engine is a LLM Engine for develo...   \n",
            "3   Yes, there is a community or support forum ava...   \n",
            "4   Yes, the Lamini library can be utilized for te...   \n",
            "5   Lamini is designed to be flexible and scalable...   \n",
            "6   You can instantiate the LLM engine using the L...   \n",
            "7   Yes, Lamini provides mechanisms for compressio...   \n",
            "8   The performance of LLMs trained using Lamini c...   \n",
            "9   Yes, there is a support and community availabl...   \n",
            "10  Yes, there are some code samples available in ...   \n",
            "11  Yes, there are some code samples available tha...   \n",
            "\n",
            "                                        target_answer  \n",
            "0   Yes, Lamini can generate technical documentati...  \n",
            "1   The Authorization HTTP header should include t...  \n",
            "2   Yes, the code includes a version parameter in ...  \n",
            "3   Yes, there is a community forum available for ...  \n",
            "4   The Lamini library is not specifically designe...  \n",
            "5   Lamini offers both free and paid plans for usi...  \n",
            "6   You can instantiate the LLM engine using the l...  \n",
            "7   Yes, Lamini provides mechanisms for model comp...  \n",
            "8   According to the information provided, Lamini ...  \n",
            "9   Yes, there is a support community available to...  \n",
            "10  Yes, the Python logging module documentation p...  \n",
            "11  Yes, there is a separate section in the docume...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset_path = \"lamini/lamini_docs_evaluation\"\n",
        "evaluation_dataset = datasets.load_dataset(evaluation_dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zcs3YNOac04",
        "outputId": "f6a0e34c-ad71-42a7-8440-d2ee927bca57"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:fsspec.local:open file: /root/.cache/huggingface/datasets/lamini___lamini_docs_evaluation/default/0.0.0/2978042a71b211bdc8ed88b3d13ed28f0554c790/dataset_info.json\n",
            "DEBUG:fsspec.local:open file: /root/.cache/huggingface/datasets/lamini___lamini_docs_evaluation/default/0.0.0/2978042a71b211bdc8ed88b3d13ed28f0554c790/dataset_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(evaluation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "oCSHoNkUagGn",
        "outputId": "57b98870-4ba0-4024-b125-283a22d22875"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:llmx:Info: LLMX_CONFIG_PATH environment variable is not set to a valid config file. Using default config file at '/usr/local/lib/python3.10/dist-packages/llmx/configs/config.default.yml'.\n",
            "INFO:llmx:Loaded config from '/usr/local/lib/python3.10/dist-packages/llmx/configs/config.default.yml'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 train\n",
              "0    {'predicted_answer': 'Yes, Lamini can generate...\n",
              "1    {'predicted_answer': 'You can use the Authoriz...\n",
              "2    {'predicted_answer': 'Laminis LLM Engine is a...\n",
              "3    {'predicted_answer': 'Yes, there is a communit...\n",
              "4    {'predicted_answer': 'Yes, the Lamini library ...\n",
              "..                                                 ...\n",
              "134  {'predicted_answer': 'Yes, Lamini has the abil...\n",
              "135  {'predicted_answer': 'I wish! This documentati...\n",
              "136  {'predicted_answer': 'Yes, Lamini can generate...\n",
              "137  {'predicted_answer': 'Yes, the documentation h...\n",
              "138  {'predicted_answer': 'Yes, you can learn to us...\n",
              "\n",
              "[139 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a831716b-eb9e-4600-96a2-50d5a598ddd8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'predicted_answer': 'Yes, Lamini can generate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'predicted_answer': 'You can use the Authoriz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'predicted_answer': 'Laminis LLM Engine is a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'predicted_answer': 'Yes, there is a communit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'predicted_answer': 'Yes, the Lamini library ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>{'predicted_answer': 'Yes, Lamini has the abil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>{'predicted_answer': 'I wish! This documentati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>{'predicted_answer': 'Yes, Lamini can generate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>{'predicted_answer': 'Yes, the documentation h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>{'predicted_answer': 'Yes, you can learn to us...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>139 rows  1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a831716b-eb9e-4600-96a2-50d5a598ddd8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a831716b-eb9e-4600-96a2-50d5a598ddd8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a831716b-eb9e-4600-96a2-50d5a598ddd8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aeab66b4-c1e9-487c-8e43-8eed1a8fa14b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aeab66b4-c1e9-487c-8e43-8eed1a8fa14b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aeab66b4-c1e9-487c-8e43-8eed1a8fa14b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "unhashable type: 'dict'"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python lm-evaluation-harness/main.py --model hf-causal --model_args pretrained=lamini/lamini_docs_finetuned --tasks arc_easy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQJdt2PXaiRW",
        "outputId": "79790628-12e2-4780-b5f6-355229551773"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-18 00:21:29.103542: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-18 00:21:29.103599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-18 00:21:29.105231: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-18 00:21:32.672673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Selected Tasks: ['arc_easy']\n",
            "Device not specified\n",
            "Cuda Available? True\n",
            "Task: arc_easy; number of docs: 2376\n",
            "Task: arc_easy; document 0; context prompt (starting on next line):\n",
            "Question: Which is the function of the gallbladder?\n",
            "Answer:\n",
            "(end of prompt on previous line)\n",
            "Requests: [Req_loglikelihood('Question: Which is the function of the gallbladder?\\nAnswer:', ' store bile')[0]\n",
            ", Req_loglikelihood('Question: Which is the function of the gallbladder?\\nAnswer:', ' produce bile')[0]\n",
            ", Req_loglikelihood('Question: Which is the function of the gallbladder?\\nAnswer:', ' store digestive enzymes')[0]\n",
            ", Req_loglikelihood('Question: Which is the function of the gallbladder?\\nAnswer:', ' produce digestive enzymes')[0]\n",
            "]\n",
            "Running loglikelihood requests\n",
            "0it [00:00, ?it/s]\n",
            "{\n",
            "  \"results\": {\n",
            "    \"arc_easy\": {\n",
            "      \"acc\": 0.3127104377104377,\n",
            "      \"acc_stderr\": 0.009512819491443737,\n",
            "      \"acc_norm\": 0.3135521885521885,\n",
            "      \"acc_norm_stderr\": 0.009519779157242258\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"arc_easy\": 0\n",
            "  },\n",
            "  \"config\": {\n",
            "    \"model\": \"hf-causal\",\n",
            "    \"model_args\": \"pretrained=lamini/lamini_docs_finetuned\",\n",
            "    \"num_fewshot\": 0,\n",
            "    \"batch_size\": null,\n",
            "    \"batch_sizes\": [],\n",
            "    \"device\": \"CUDA\",\n",
            "    \"no_cache\": false,\n",
            "    \"limit\": null,\n",
            "    \"bootstrap_iters\": 100000,\n",
            "    \"description_dict\": {}\n",
            "  }\n",
            "}\n",
            "hf-causal (pretrained=lamini/lamini_docs_finetuned), limit: None, provide_description: False, num_fewshot: 0, batch_size: None\n",
            "|  Task  |Version| Metric |Value |   |Stderr|\n",
            "|--------|------:|--------|-----:|---|-----:|\n",
            "|arc_easy|      0|acc     |0.3127|  |0.0095|\n",
            "|        |       |acc_norm|0.3136|  |0.0095|\n",
            "\n"
          ]
        }
      ]
    }
  ]
}